import numpy as np
import pandas as pd
import seaborn as sns
import scipy.stats as stats
import matplotlib.pyplot as plt
from sklearn.preprocessing import PowerTransformer

sns.set_theme()
sns.set_palette(palette="rainbow")


class BNT:
    """Biolog Normalization Tool which employs a Yeo-Johnson normalization approach to analyse values generated by BIOLOG Plate Reader. Intended to be used with BIOLOG AN Plates results."""

    def __init__(self, dataframe: pd.DataFrame) -> None:
        """Initiates Biolog Normalization Tool. The raw BIOLOG results dataframe must be defined. Intended to be used with tool "Cleaner" tool.

        Parameters
        ----------
        dataframe : pd.DataFrame
            Raw BIOLOG results dataframe. "Cleaner" tool´s "cleaNorganize" method returns the dataframe with the required structure.
        """
        self.df = dataframe

        self.calculation_dif()
        self.yeojohn_normalization(self.dif_df)
        self.calculation_ave()
        self.add_ave_dataframe()

    def calculation_dif(self) -> pd.DataFrame:
        """Calculates columnwise the difference of each value to the negative control ("Water" as blank) which is the first value of the column.

        Returns
        -------
        pd.DataFrame
            Return a pandas dataframe containing the calculated difference of the results to their corresponding negative control ("Water" as blank),
        """
        # Define variables for the process
        self.dif_df = pd.DataFrame(index=self.df.index, columns=self.df.columns)

        # Use the information of the dataset hierarchy to calculate the difference of each value to the "Water" value of each column
        samples = self.df.columns.get_level_values(level=0).unique()
        for sample in samples:
            tps = self.df.loc[:, sample].columns.get_level_values(level=0).unique()
            for tp in tps:
                plates = (
                    self.df.loc[:, (sample, tp)]
                    .columns.get_level_values(level=0)
                    .unique()
                )
                for plate in plates:
                    dif = (
                        self.df.loc[:, (sample, tp, plate)]
                        - self.df.loc["Water", (sample, tp, plate)]
                    )
                    self.dif_df[(sample, tp, plate)] = dif

        self.dif_df = self.dif_df.sort_index(axis=1)

        return self.dif_df

    def calculation_ave(self) -> pd.DataFrame:
        """Calculates the average value for each timepoint using the 3 plates´ values.

        Returns
        -------
        pd.DataFrame
            Returns a pandas dataframe containing the average value of each timepoint for each sample.
        """
        # Define variables for the process
        self.tp_tuples = []

        # Use the information of the hierarchy system to define the tuple list and dictionary
        samples = self.df.columns.get_level_values(level=0).unique()
        for sample in samples:
            tps = self.df.loc[:, sample].columns.get_level_values(level=0).unique()
            for tp in tps:
                self.tp_tuples.append((sample, tp))

        # Create MultiIndex columns
        self.multi_columns = pd.MultiIndex.from_tuples(
            self.tp_tuples, names=["Sample #", "TP"]
        )

        # Create empty dataframe with correct index and columns
        self.ave_df = pd.DataFrame(index=self.df.index, columns=self.multi_columns)

        # Calculate the average of plates for each row in the dataset
        samples = self.ave_df.columns.get_level_values(level=0).unique()
        for sample in samples:
            tps = self.ave_df.loc[:, sample].columns.get_level_values(level=0).unique()
            for tp in tps:
                tp_average = self.normalized_df.loc[:, (sample, tp)].mean(axis=1)
                self.ave_df[sample, tp] = tp_average

        # Sort and round columns of resulting dataframe
        self.ave_df = self.ave_df.sort_index(axis=1)
        # self.ave_df = self.ave_df.round(2)

        # Returns dataframe with the average of the plates for each timepoint of each sample
        return self.ave_df

    def add_ave_dataframe(self) -> pd.DataFrame:
        """Adds the calculated average from method "calculation_ave" of each timepoint to their respective position in the dataframe.

        Returns
        -------
        pd.DataFrame
            Returns a pandas dataframe containing the difference of the values to their blank value along with their average under column "Plate Avg."
        """
        self.complete_df = self.normalized_df.copy()

        samples = self.complete_df.columns.get_level_values(level=0).unique()
        for sample in samples:
            tps = (
                self.complete_df.loc[:, sample]
                .columns.get_level_values(level=0)
                .unique()
            )
            for tp in tps:
                self.complete_df.loc[:, (sample, tp, "Plate Avg.")] = self.ave_df.loc[
                    :, (sample, tp)
                ]

        self.complete_df = self.complete_df.sort_index(axis=1)

        return self.complete_df

    def yeojohn_normalization(self, dataframe) -> pd.DataFrame:
        """Method transforms the complete difference dataframe optained from methods "calculation_dif", "calculation_ave" and "add_ave_dataframe" by normalizing the data per column using the Yeo-Johnson approach.

        Returns
        -------
        pd.DataFrame
            Returns a dataframe containing the normalized data.
        """

        target_df = dataframe

        self.norm_dict = dict()

        yeojohnTr = PowerTransformer(
            standardize=True
        )  # yeo-johnson transformation is the default method attribute

        for column in target_df.columns:
            norm_data = yeojohnTr.fit_transform(target_df[column].values.reshape(-1, 1))

            norm_data = norm_data.reshape(1, len(norm_data))[0]

            self.norm_dict[column] = norm_data

        self.normalized_df = pd.DataFrame.from_dict(self.norm_dict)

        self.normalized_df.index = target_df.index

        return self.normalized_df

    def generate_skewness_data(self) -> pd.DataFrame:
        """Gathers the skewness value of each dataset (columnwise) before and after normalization using the Yeo-Johnson approach.

        Returns
        -------
        pd.DataFrame
            Returns a pandas dataframe containing the skewness information before and after normalization for each defined dataset.
        """
        old_skew = self.dif_df.skew()
        new_skew = self.normalized_df.skew()

        skew_dict = {"Before Trans.": old_skew, "After Trans.": new_skew}

        self.skew_df = pd.DataFrame.from_dict(skew_dict)
        self.skew_df.Name = "Skewness Information"

        return self.skew_df

    def identify_result_type(
        self,
        dataframe: pd.DataFrame,
        limit_i: float = 1.0,
        limit_b: float = 1.5,
        std: float = 1.005249,
    ) -> pd.DataFrame:
        tags = ["I", "B", "P"]

        target_df = dataframe

        self.id_df = pd.DataFrame(index=target_df.index, columns=target_df.columns)

        samples = target_df.columns.get_level_values(level=0).unique()
        for sample in samples:
            tps = self.id_df.loc[:, sample].columns.get_level_values(level=0).unique()
            for tp in tps:
                plates = (
                    self.id_df.loc[:, (sample, tp)]
                    .columns.get_level_values(level=0)
                    .unique()
                )
                for plate in plates:
                    target = target_df.loc[:, (sample, tp, plate)]

                    conditions = [
                        (target <= limit_i * std),
                        (target > limit_i * std) & (target <= limit_b * std),
                        (target > limit_b * std),
                    ]
                    self.id_df[(sample, tp, plate)] = np.select(
                        conditions, tags, default="NaN"
                    )

        return self.id_df

    def conditional_formatting(
        self,
        cell,
        limit_i: float,
        limit_b: float,
        bg_color_bdry: str,
        hl_color_bdry: str,
        bg_color_pos: str,
        hl_color_pos: str,
        std: float = 1.005249,
    ):

        if cell <= limit_i * std:
            return "background-color: white; color: darkgrey"
        elif cell > limit_i * std and cell <= limit_b * std:
            return f"background-color: {bg_color_bdry}; font-weight: bold; color: {hl_color_bdry}"
        else:
            return f"background-color: {bg_color_pos}; font-weight: bold; color: {hl_color_pos}"

    def style_categories(
        self,
        column,
        limit_i: float,
        limit_b: float,
        bg_color_bdry: str,
        hl_color_bdry: str,
        bg_color_pos: str,
        hl_color_pos: str,
    ):
        """Conditional formatting function utilized by method "display_categories". The 3 possible categories for any value are "indifferent", "boundary" and "positive". Values are color coded depending to which category they fall into based on a percentual value of the normalized data´s standard deviation.

        Parameters
        ----------
        column : _type_
            Input column variable attained through pandas Style function "apply".
        limit_i : float
            Percentage as a decimal value used to define the "indifferent" category limit. This value represent the percentual amount of the standard deviation (1.0 = 100%).
        limit_b : float
            Percentage as a decimal value used to define the "boundary" category limit. This value represent the percentual amount of the standard deviation (1.0 = 100%).
        bg_color_bdry : str
            Background color for values categorized as "boundary".
        hl_color_bdry : str
            Highlight color for values categorized as "boundary".
        bg_color_pos : str
            Background color for values categorized as "positive".
        hl_color_pos : str
            Highlight color for values categorized as "positive".

        Returns
        -------
        _type_
            Conditional formatting for analyzed cell.
        """
        result = []

        for cell in column:
            cf = self.conditional_formatting(
                limit_i=limit_i,
                limit_b=limit_b,
                bg_color_bdry=bg_color_bdry,
                hl_color_bdry=hl_color_bdry,
                bg_color_pos=bg_color_pos,
                hl_color_pos=hl_color_pos,
                cell=cell,
            )
            result.append(cf)

        return result

    def display_categories(
        self,
        dataframe: pd.DataFrame,
        limit_i: float = 1.0,
        limit_b: float = 1.5,
        bg_color_bdry: str = "bisque",
        hl_color_bdry: str = "darkorange",
        bg_color_pos: str = "paleturquoise",
        hl_color_pos: str = "steelblue",
    ):
        """Displays normalized data with color categorization depending on defined limit values.

        Parameters
        ----------
        dataframe : pd.DataFrame
            Target pandas dataframe which should be displayed with the categorization.
        limit_i : float, optional
            Percentage as a decimal value used to define the "indifferent" category limit. This value represent the percentual amount of the standard deviation (1.0 = 100%). Default is 1.0.
        limit_b : int, optional
            Percentage as a decimal value used to define the "boundary" category limit. This value represent the percentual amount of the standard deviation (1.0 = 100%). Anything within "limit_i" and "limit_b" is considered "boundary". Default is 1.5.
        bg_color_bdry : str, optional
            Background color for values categorized as "boundary". Default is "bisque".
        hl_color_bdry : str, optional
            Highlight color for values categorized as "boundary". Default is "darkorange".
        bg_color_pos : str, optional
            Background color for values categorized as "positive". Default is "paleturquoise".
        hl_color_pos : str, optional
            Highlight color for values categorized as "positive". Default is "steelblue".

        Returns
        -------
        _type_
            Returns the dataframe with the conditional formatting applied.
        """
        result = (
            dataframe.style.set_table_attributes("style='display:inline'")
            .apply(
                self.style_categories,
                limit_i=limit_i,
                limit_b=limit_b,
                bg_color_bdry=bg_color_bdry,
                hl_color_bdry=hl_color_bdry,
                bg_color_pos=bg_color_pos,
                hl_color_pos=hl_color_pos,
                axis=1,
            )
            .format(precision=2)
        )

        return result

    def display_yeojohn_norm(self, target_columns: str = None):
        """Method shows the normalization effect on the data when applying the Yeo-Johnson transformation. Displays a histogram and QQ-plot for each dataset before and after the normalization.

        Parameters
        ----------
        target_columns : str, optional
            Target columns for which the histograms and QQ-plot should be displayed.
        """
        self.calculation_dif()
        self.calculation_ave()
        self.add_ave_dataframe()
        self.yeojohn_normalization(dataframe=self.dif_df)

        if target_columns != None:
            target_norm_df = self.normalized_df[target_columns]
            target_dif_df = self.dif_df[target_columns]

            for column in target_norm_df:
                plt.figure(figsize=(30, 6))

                plt.subplot(1, 4, 1)
                plt.title("Distribution before Transformation", fontsize=15)
                sns.histplot(target_dif_df[column], kde=True, color="red")

                plt.subplot(1, 4, 2)
                stats.probplot(target_dif_df[column], dist="norm", plot=plt)  # QQ Plot
                plt.title("QQ Plot before Transformation", fontsize=15)

                plt.subplot(1, 4, 3)
                plt.title("Distribution after Transformation", fontsize=15)
                sns.histplot(target_norm_df[column], bins=20, kde=True, legend=False)

                plt.subplot(1, 4, 4)
                stats.probplot(target_norm_df[column], dist="norm", plot=plt)  # QQ Plot
                plt.title("QQ Plot after Transformation", fontsize=15)

                plt.show()

        else:
            for column in self.normalized_df.columns:
                plt.figure(figsize=(30, 6))

                plt.subplot(1, 4, 1)
                plt.title("Distribution before Transformation", fontsize=15)
                sns.histplot(self.complete_df[column], kde=True, color="red")

                plt.subplot(1, 4, 2)
                stats.probplot(
                    self.complete_df[column], dist="norm", plot=plt
                )  # QQ Plot
                plt.title("QQ Plot before Transformation", fontsize=15)

                plt.subplot(1, 4, 3)
                plt.title("Distribution after Transformation", fontsize=15)
                sns.histplot(
                    self.normalized_df[column], bins=20, kde=True, legend=False
                )

                plt.subplot(1, 4, 4)
                stats.probplot(
                    self.normalized_df[column], dist="norm", plot=plt
                )  # QQ Plot
                plt.title("QQ Plot after Transformation", fontsize=15)

                plt.show()

    def identify_best_boundaries(
        self, cat_df: pd.DataFrame, target_df: pd.DataFrame
    ) -> pd.DataFrame:

        # Definition of variables

        ## Iteration variable of the available samples
        df_samples = cat_df.columns.get_level_values(level=0).unique()

        ## Iteration variable of the defined column names
        col_names = [x for x in cat_df.columns.names]

        ## Dataframe for results
        results_df = pd.DataFrame(columns=cat_df.columns, index=cat_df.index)

        ## Empty lists
        idx_list = []
        res_list = []

        # Process

        ## Iteration of the lower bound
        for ib in np.arange(0.0, 1.5, 0.1):
            ib = round(ib.tolist(), 1)

            ## Iteration of the upper bound
            for bp in np.arange(1.6, 2.5, 0.1):

                bp = round(bp.tolist(), 1)

                ## Create tuple containing the combination of the lower (lb) and upper (ub) bounds
                idx_list.append((ib, bp))

                ## Run the identification of the assigned category based on newly created bounds
                self.identify_result_type(dataframe=target_df, limit_i=ib, limit_b=bp)

                ## Reorganized df structure to best suit the function, removed "Plate Avg." from each sample
                id_tgt_df = self.id_df.loc[
                    :, (df_samples, slice(None), ["Plate 1", "Plate 2", "Plate 3"])
                ]

                ## Renaming columns to original names
                id_tgt_df.columns = id_tgt_df.columns.set_names(col_names)

                ## Run comparison between new, normalized cat. with original cat.
                comp_df = id_tgt_df.compare(cat_df, keep_shape=True).fillna(0)

                ## Multiindex sorting
                comp_samples = comp_df.columns.get_level_values(level=0).unique()
                for sample in comp_samples:
                    tps = (
                        comp_df.loc[:, sample]
                        .columns.get_level_values(level=0)
                        .unique()
                    )
                    for tp in tps:
                        plates = (
                            comp_df.loc[:, (sample, tp)]
                            .columns.get_level_values(level=0)
                            .unique()
                        )
                        for plate in plates:
                            for idx in comp_df.index:

                                ## Coordinates of solutions in comp. df
                                loc_self = (sample, tp, plate, "self")
                                loc_other = (sample, tp, plate, "other")

                                ## Identification of matches for each datapoint categories
                                results_df.loc[idx, (sample, tp, plate)] = np.where(
                                    comp_df.loc[idx, loc_self]
                                    == comp_df.loc[idx, loc_other],
                                    "YES",  # Matching categories
                                    "NO",  # No matching categories
                                )

                ## Iteration counting matches per Sample->TP->Plate and establish accuracy percentage
                idx_res_list = []
                for col in results_df.columns:
                    dp = results_df[col].value_counts()
                    dp.index = [value.tolist() for value in dp.index.values]

                    result = round((dp["YES"] / len(cat_df)) * 100, 2).item()

                    ## Gather solution percentages
                    idx_res_list.append(result)

                ## Gather all solutions per boundary combination
                res_list.append(idx_res_list)

        ## Creation of best solution dataframe
        best_sol_dict = {idx_list[i]: res_list[i] for i in range(len(idx_list))}
        best_sol_df = pd.DataFrame.from_dict(
            best_sol_dict, orient="index", columns=cat_df.columns
        )

        return best_sol_df
